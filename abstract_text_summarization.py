# -*- coding: utf-8 -*-
"""Abstract_Text_summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V1g_iQTxS8brz9KxFfTLAG250zPtiCwm
"""

import pandas as pd
import numpy as np
import re   #regular expression support,build in

AMAZON_DATA_PATH= '/content/Reviews.csv'

class Load_amazon_data:

    def __init__(self, dir_path, seed= 0):

        #Initialization

        self.dir_path= dir_path
        np.random.seed(seed)

    def load(self):

        #Reads data from the given directory path

        return pd.read_csv(self.dir_path)

    def drop(self):

        #Drops unnecessary columns

        data= self.load()

        data = data.dropna()
        data= data.iloc[:, -2:]
        data = data.reset_index(drop= True)

        return data

    def analyze_data(self):

        #Prints some sample data points from the cleaned data

        data= self.drop()

        for sr_no, i in enumerate(np.random.randint(10, 100, size= 5)):
            print("_________________________")
            print("Data Point {0}".format(sr_no + 1))
            print("Summary:")
            print(data['Summary'].iloc[i])
            print("Full Text:")
            print(data['Text'].iloc[i])

obj= Load_amazon_data(AMAZON_DATA_PATH, seed= 1)

data= obj.load()
data.head()

#Droping unnessesary columns
data= obj.drop()
data.head()

data.shape

obj.analyze_data()    #Data analyze

contractions = {
"ain't": "am not",
"aren't": "are not",
"can't": "cannot",
"can't've": "cannot have",
"'cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he would",
"he'd've": "he would have",
"he'll": "he will",
"he's": "he is",
"how'd": "how did",
"how'll": "how will",
"how's": "how is",
"i'd": "i would",
"i'll": "i will",
"i'm": "i am",
"i've": "i have",
"isn't": "is not",
"it'd": "it would",
"it'll": "it will",
"it's": "it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"must've": "must have",
"mustn't": "must not",
"needn't": "need not",
"oughtn't": "ought not",
"shan't": "shall not",
"sha'n't": "shall not",
"she'd": "she would",
"she'll": "she will",
"she's": "she is",
"should've": "should have",
"shouldn't": "should not",
"that'd": "that would",
"that's": "that is",
"there'd": "there had",
"there's": "there is",
"they'd": "they would",
"they'll": "they will",
"they're": "they are",
"they've": "they have",
"wasn't": "was not",
"we'd": "we would",
"we'll": "we will",
"we're": "we are",
"we've": "we have",
"weren't": "were not",
"what'll": "what will",
"what're": "what are",
"what's": "what is",
"what've": "what have",
"where'd": "where did",
"where's": "where is",
"who'll": "who will",
"who's": "who is",
"won't": "will not",
"wouldn't": "would not",
"you'd": "you would",
"you'll": "you will",
"you're": "you are"
}

class Data_cleaning:
    def __init__(self):
        self.clean_summaries= []
        self.clean_texts= []

    def clean_text(self, text, remove_stopwords = False):

        #Defines a series of cleaning operations

        text = text.lower()

        if True:
            text = text.split()
            new_text = []
            for word in text:
                if word in contractions:
                    new_text.append(contractions[word])
                else:
                    new_text.append(word)
            text = " ".join(new_text)

        text = re.sub(r'https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
        text = re.sub(r'\<a href', ' ', text)
        text = re.sub(r'&amp;', '', text)
        text = re.sub(r'[_"\-;%()|+&=*%.!,?:#$@\[\]/]', ' ', text)
        text = re.sub(r'<br />', ' ', text)
        text = re.sub(r'<br >', ' ', text)
        text = re.sub(r'<br  >', ' ', text)
        text = re.sub(r'\'', ' ', text)

        # Optionally, remove stop words(unimportant words)
        if remove_stopwords:
            text = text.split()
            stops = set(stopwords.words("english"))
            text = [w for w in text if not w in stops]
            text = " ".join(text)

        return text

    def clean(self, data):

        #Applies the clean_text() to the entire dataset
        for summary in data.Summary:
            self.clean_summaries.append(self.clean_text(summary))

        print("Summaries are complete.")

        for text in data.Text:
            self.clean_texts.append(self.clean_text(text))

        print("Texts are complete.")

        return self.clean_summaries, self.clean_texts

clean_obj= Data_cleaning()
clean_summaries, clean_texts= clean_obj.clean(data)

np.random.seed(1)   #Display random points

for sr_no, i in enumerate(np.random.randint(10, 100, size= 5)):
    print("_________________________")
    print("Data Point #{0}".format(sr_no + 1))
    print("Summary:")
    print(clean_summaries[i])
    print("Full Text:")
    print(clean_texts[i])

amazon_data= list()   #append new data
for (summ, story) in zip(clean_summaries, clean_texts):
    amazon_data.append({'story': [story], 'highlights':[summ]})

for sr_no, i in enumerate(np.random.randint(10, 10000, size= 5)):
    print("_________________________")
    print("Data Point #{0}".format(sr_no + 1))
    print("Summary:")
    print(amazon_data[i]['highlights'])
    print("Full Text:")
    print(amazon_data[i]['story'])

from pickle import dump,load

dump(amazon_data, open('/content/text_amazon.data', 'wb'))

from pickle import dump,load

amazon_data= load(open('/content/text_amazon.data', 'rb'))
amazon_data[0]

amazon_data[5]

len(amazon_data)

#counts the occurance of each word
def count_words(count_dict, text):

    for word in text.split():
        if word not in count_dict:
            count_dict[word] = 1
        else:
            count_dict[word] += 1

word_counts = {}
for data_point in amazon_data:
    count_words(word_counts, data_point['highlights'][0])
    count_words(word_counts, data_point['story'][0])

print("Size of Vocabulary:", len(word_counts))

print(word_counts)

import numpy as np
embeddings_index = {}
with open("/content/sample-2mb-text-file.txt") as f:
    for line in f:
        values = line.split(' ')
        word = values[0]
        embedding = np.array(values[1:])
        embeddings_index[word] = embedding

print('Word embeddings:', len(embeddings_index))

embeddings_index

# Find the number of words that are missing from CN, and are used more than our threshold.
missing_words = 0
threshold = 20

for word, count in word_counts.items():
    if count > threshold:
        if word not in embeddings_index:
            missing_words += 1

missing_ratio = round(missing_words/len(word_counts),5)*100

print("Number of words missing from CN:", missing_words)
print("Percent of words that are missing from vocabulary: {0}%".format(missing_ratio))

word_counts

#dictionary to convert words to integers
vocab_to_int = {}

value = 0
for word, count in word_counts.items():
    if count >= threshold or word in embeddings_index:
        vocab_to_int[word] = value
        value += 1

# Special tokens that will be added to our vocab
codes = ["<UNK>","<PAD>","<EOS>","<GO>"]

# Add codes to vocab
for code in codes:
    vocab_to_int[code] = len(vocab_to_int)

# Dictionary to convert integers to words
int_to_vocab = {}
for word, value in vocab_to_int.items():
    int_to_vocab[value] = word

usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100

print("Total number of unique words:", len(word_counts))
print("Number of words we will use:", len(vocab_to_int))
print("Percent of words we will use: {}%".format(usage_ratio))

# Need to use 300 for embedding dimensions to match CN's vectors.
embedding_dim = 300
nb_words = len(vocab_to_int)

word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)
for word, i in vocab_to_int.items():
    if word in embeddings_index:
        word_embedding_matrix[i] = embeddings_index[word]
    else:
        # If word not in CN, create a random embedding for it
        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))
        embeddings_index[word] = new_embedding
        word_embedding_matrix[i] = new_embedding

print(len(word_embedding_matrix))

word_embedding_matrix.shape

def convert_to_ints(sentence, word_count, unk_count, eos=False):

    sentence_ints = []
    for word in sentence.split():
        word_count += 1
        if word in vocab_to_int:
            sentence_ints.append(vocab_to_int[word])
        else:
            sentence_ints.append(vocab_to_int["<UNK>"])
            unk_count += 1
    if eos:
        sentence_ints.append(vocab_to_int["<EOS>"])
    return sentence_ints, word_count, unk_count

word_count = 0
unk_count = 0

int_summaries= list()
int_texts= list()

for data_point in amazon_data:
    summaries, word_count, unk_count = convert_to_ints(data_point['highlights'][0], word_count, unk_count)
    texts, word_count, unk_count = convert_to_ints(data_point['story'][0], word_count, unk_count, eos=True)
    int_summaries.append(summaries)
    int_texts.append(texts)


unk_percent = round(unk_count/word_count,4)*100

print("Total number of words in headlines:", word_count)
print("Total number of UNKs in headlines:", unk_count)
print("Percent of words that are UNK: {}%".format(unk_percent))

import pandas as pd
def create_lengths(text):
    '''Create a data frame of the sentence lengths from a text'''
    lengths = []
    for sentence in text:
        lengths.append(len(sentence))
    return pd.DataFrame(lengths, columns=['counts'])

lengths_summaries = create_lengths(int_summaries)
lengths_texts = create_lengths(int_texts)

lengths_texts.head()

print("Summaries:")
print(lengths_summaries.describe())
print("Texts:")
print(lengths_texts.describe())

print(lengths_summaries.shape)
lengths_summaries

# Inspect the length of texts
print(np.percentile(lengths_texts.counts, 90))
print(np.percentile(lengths_texts.counts, 95))
print(np.percentile(lengths_texts.counts, 99))
print(np.percentile(lengths_texts.counts, 10))

# Inspect the length of summaries
print(np.percentile(lengths_summaries.counts, 90))
print(np.percentile(lengths_summaries.counts, 95))
print(np.percentile(lengths_summaries.counts, 99))
print(np.percentile(lengths_summaries.counts, 10))

def unk_counter(sentence):
    '''Counts the number of time UNK appears in a sentence.'''
    unk_count = 0
    for word in sentence:
        if word == vocab_to_int["<UNK>"]:
            unk_count += 1
    return unk_count

min(lengths_texts.counts)

#sort the summaries
sorted_summaries = []
sorted_texts = []
max_text_length = 161
max_summary_length = 8
summ_min_length = 2
text_min_length= 25
unk_text_limit = 1
unk_summary_limit = 0

for length in range(min(lengths_texts.counts), max_text_length):
    for count, words in enumerate(int_summaries):
        if (len(int_summaries[count]) >= summ_min_length and
            len(int_texts[count]) >= text_min_length and
            len(int_summaries[count]) <= max_summary_length and
            unk_counter(int_summaries[count]) <= unk_summary_limit and
            unk_counter(int_texts[count]) <= unk_text_limit and
            length == len(int_texts[count])  #SO that points are not repeated
           ):

            sorted_summaries.append(int_summaries[count])
            sorted_texts.append(int_texts[count])

# Compare lengths to ensure they match
print(len(sorted_summaries))
print(len(sorted_texts))

sorted_texts

len(vocab_to_int)

dump(sorted_texts, open('/content/text_amazon.data', 'wb'))
dump(sorted_summaries, open('/content/sample-2mb-text-file.txt', 'wb'))

index= 100

sample_text= sorted_texts[index]
sample_summary= sorted_summaries[index]

sent= ""
for word in sample_text:
    word= int_to_vocab[word]
    sent += ' ' + word

sent

summ= ""
for word in sample_summary:
    summ += int_to_vocab[word] + " "

summ

#Padding
max_text_length= 217
max_summary_length= 9 + 1

def transform_input_text(texts):
    padded_texts= list()
    for text in texts:
        if len(text) < max_text_length:
            text= text + [vocab_to_int['<PAD>'] for i in range(max_text_length - len(text))]

        else:
            text= text[:max_text_length]
        padded_texts.append(text)

    return padded_texts

padded_sorted_texts= transform_input_text(sorted_texts)
padded_sorted_summaries= sorted_summaries

len(padded_sorted_texts)

word_embedding_matrix.shape

import matplotlib.pyplot as plt
import pandas as pd

text_word_count = []
summary_word_count = []


for i in clean_texts:
      text_word_count.append(len(i.split()))

for i in clean_summaries:
      summary_word_count.append(len(i.split()))

a = {'text':text_word_count, 'summary':summary_word_count}
length_df = pd.DataFrame.from_dict(a, orient='index')
length_df = length_df.transpose()

length_df.hist(bins = 30)
plt.show()

